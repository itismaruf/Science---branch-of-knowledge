import pandas as pd
import numpy as np
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
import plotly.express as px
import re
from AI_helper import get_chatgpt_response

def load_data(uploaded_file):
    df = pd.read_csv(uploaded_file)
    conversion_log = []

    for col in df.columns:
        original_dtype = df[col].dtype

        if original_dtype == "object":
            # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –∑–∞–ø—è—Ç—ã–µ
            df[col] = df[col].astype(str).str.replace(",", ".").str.strip()

            # –ü—Ä–æ–±—É–µ–º –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —á–∏—Å–ª—É
            try:
                df[col] = pd.to_numeric(df[col], errors="raise")
                conversion_log.append(f"{col}: object ‚Üí float (—É—Å–ø–µ—à–Ω–æ)")
            except:
                try:
                    df[col] = pd.to_numeric(df[col], errors="coerce")
                    success_rate = df[col].notnull().mean()
                    if success_rate > 0.9:
                        conversion_log.append(f"{col}: object ‚Üí float (–ø–æ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ {success_rate:.0%})")
                    else:
                        df[col] = df[col].astype(str)
                        conversion_log.append(f"{col}: –æ—Å—Ç–∞–ª—Å—è –∫–∞–∫ —Ç–µ–∫—Å—Ç (—Ç–æ–ª—å–∫–æ {success_rate:.0%} —á–∏—Å–µ–ª)")
                except:
                    df[col] = df[col].astype(str)
                    conversion_log.append(f"{col}: –æ—Å—Ç–∞–ª—Å—è –∫–∞–∫ —Ç–µ–∫—Å—Ç (–Ω–µ–ø–æ–¥–¥–∞—é—â–∏–π—Å—è —Ñ–æ—Ä–º–∞—Ç)")
        else:
            conversion_log.append(f"{col}: {original_dtype} (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–∏–ø–æ–≤
    st.session_state["conversion_log"] = conversion_log

    # üìä –§–æ—Ä–º–∏—Ä—É–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
    base_info = {
        "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫": df.shape[0],
        "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–æ–ª–±—Ü–æ–≤": df.shape[1],
        "–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è": int(df.isnull().sum().sum()),
        "–î—É–±–ª–∏–∫–∞—Ç—ã": int(df.duplicated().sum()),
        "–ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏": len(df.select_dtypes(include=["number"]).columns),
        "–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏": len(df.select_dtypes(include=["object"]).columns),
    }

    st.session_state["base_info"] = base_info
    st.subheader("üìä –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö")
    for key, value in st.session_state["base_info"].items():
        st.markdown(f"- **{key}:** {value}")

    return df


def summarize_data(df):
    summary = f"""
### ‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ

- üî¢ **–†–∞–∑–º–µ—Ä:** {df.shape[0]} —Å—Ç—Ä–æ–∫, {df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤
- üìÑ **–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:**
{df.dtypes.to_string()}

- üß© **–ü—Ä–æ–ø—É—Å–∫–∏:**
{df.isnull().sum()[df.isnull().sum() > 0].to_string() if df.isnull().sum().sum() > 0 else '–ù–µ—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤'}

- üß™ **–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:**
{df.head(3).to_markdown()}
"""
    return summary


def summarize_columns_for_gpt(df):
    summary = []
    for col in df.columns:
        null_pct = round(df[col].isnull().mean() * 100, 1)
        examples = df[col].dropna().astype(str).unique()[:3]
        summary.append({
            "column": col,
            "type": str(df[col].dtype),
            "nulls": null_pct,
            "examples": list(examples)
        })
    return summary


def ask_gpt_smart_cleaning(summary):
    prompt = (
        "–ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —É–∫–∞–∂–∏, –∫–∞–∫–∏–µ —Å—Ç–æ–ª–±—Ü—ã –Ω—É–∂–Ω–æ –∑–∞–ø–æ–ª–Ω–∏—Ç—å –∏ —á–µ–º "
        "(mean, median –∏–ª–∏ mode), –∞ –∫–∞–∫–∏–µ –Ω–µ —Ç—Ä–æ–≥–∞—Ç—å. –§–æ—Ä–º–∞—Ç:\n\n"
        "- –ó–∞–ø–æ–ª–Ω–∏—Ç—å: age: mean, city: mode\n"
        "- –ù–µ —Ç—Ä–æ–≥–∞—Ç—å: country, code\n\n"
        "–¢–æ–ª—å–∫–æ —Å–ø–∏—Å–æ–∫, –±–µ–∑ –ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤.\n\n"
        f"{summary}"
    )
    return get_chatgpt_response(prompt)  # —Ç—É—Ç –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å


def apply_gpt_cleaning(df, gpt_response):
    import re
    to_fill = {}
    do_not_touch = []

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π GPT
    fill_part = re.search(r"- –ó–∞–ø–æ–ª–Ω–∏—Ç—å:\s*(.*?)(?:\n|$)", gpt_response)
    if fill_part:
        for item in fill_part.group(1).split(','):
            parts = item.strip().split(':')
            if len(parts) == 2:
                col, method = parts[0].strip(), parts[1].strip()
                if col in df.columns:
                    to_fill[col] = method

    notouch_part = re.search(r"- –ù–µ —Ç—Ä–æ–≥–∞—Ç—å:\s*(.*)", gpt_response)
    if notouch_part:
        do_not_touch = [x.strip() for x in notouch_part.group(1).split(',') if x.strip() in df.columns]

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –ª–æ–≥
    cleaning_log = []

    for col in df.columns:
        if col in to_fill:
            method = to_fill[col]
            if df[col].isnull().sum() > 0:
                try:
                    if method == "mean":
                        df[col].fillna(df[col].mean(), inplace=True)
                        cleaning_log.append(f"{col}: –∑–∞–ø–æ–ª–Ω–µ–Ω–æ (mean)")
                    elif method == "median":
                        df[col].fillna(df[col].median(), inplace=True)
                        cleaning_log.append(f"{col}: –∑–∞–ø–æ–ª–Ω–µ–Ω–æ (median)")
                    elif method == "mode":
                        mode_val = df[col].mode()
                        if not mode_val.empty:
                            df[col].fillna(mode_val[0], inplace=True)
                            cleaning_log.append(f"{col}: –∑–∞–ø–æ–ª–Ω–µ–Ω–æ (mode)")
                        else:
                            cleaning_log.append(f"{col}: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–æ–ª–Ω–∏—Ç—å (–ø—É—Å—Ç–æ–π mode)")
                    else:
                        cleaning_log.append(f"{col}: –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è ({method})")
                except Exception as e:
                    cleaning_log.append(f"{col}: –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–∏ ‚Üí {str(e)}")
            else:
                cleaning_log.append(f"{col}: –ø—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–µ—Ç, –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ")
        elif col in do_not_touch:
            cleaning_log.append(f"{col}: –æ—Å—Ç–∞–≤–ª–µ–Ω –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π")
        else:
            if df[col].isnull().sum() > 0:
                cleaning_log.append(f"{col}: —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–æ–ø—É—Å–∫–∏, –Ω–æ –Ω–µ —É–∫–∞–∑–∞–Ω –≤ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö")
            else:
                cleaning_log.append(f"{col}: –±–µ–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥ –≤ —Å–µ—Å—Å–∏—é –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    st.session_state["cleaning_log"] = cleaning_log
    return cleaning_log


# –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö

def analyze_data_quality(df: pd.DataFrame) -> str:
    total_rows = len(df)
    total_cols = len(df.columns)
    missing_total = df.isnull().sum().sum()
    duplicated_rows = df.duplicated().sum()
    dtypes_summary = df.dtypes.value_counts().to_dict()

    dtype_text = ", ".join([f"{v} —Å—Ç–æ–ª–±—Ü–æ–≤ —Å —Ç–∏–ø–æ–º {k}" for k, v in dtypes_summary.items()])

    summary = f"""
‚úÖ –í –≤–∞—à–µ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö {total_rows} —Å—Ç—Ä–æ–∫ –∏ {total_cols} —Å—Ç–æ–ª–±—Ü–æ–≤.

üßº –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö:
- –ë—ã–ª–æ —É–¥–∞–ª–µ–Ω–æ {duplicated_rows} –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö —Å—Ç—Ä–æ–∫.
- –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {'–Ω–µ—Ç' if missing_total == 0 else missing_total}.

üîé –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö: {dtype_text}.

üéâ –î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã –∫ –¥–∞–ª—å–Ω–µ–π—à–µ–º—É –∞–Ω–∞–ª–∏–∑—É –∏ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–∏.
"""
    return summary


# –û—Ç—á—ë—Ç –æ–± –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏

def generate_data_cleaning_report(df_original, df_cleaned):
    report = []

    rows_before = df_original.shape[0]
    rows_after = df_cleaned.shape[0]
    dups_removed = df_original.duplicated().sum()
    rows_removed = rows_before - rows_after

    report.append(f"- üßæ –ò—Å—Ö–æ–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫: **{rows_before}**, –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: **{rows_after}**")
    report.append(f"- ‚ùå –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: **{dups_removed}**")
    report.append(f"- üßπ –í—Å–µ–≥–æ —É–¥–∞–ª–µ–Ω–æ —Å—Ç—Ä–æ–∫: **{rows_removed}** (–¥—É–±–ª–∏–∫–∞—Ç—ã + —Å—Ç—Ä–æ–∫–∏ —Å >50% NaN)")

    nan_percent = df_original.isnull().mean().round(2) * 100
    if nan_percent[nan_percent > 0].empty:
        report.append("- ‚úÖ –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –¥–æ –æ—á–∏—Å—Ç–∫–∏ –Ω–µ –±—ã–ª–æ.")
    else:
        report.append("### üîç –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–æ –æ—á–∏—Å—Ç–∫–∏:")
        report.append(nan_percent[nan_percent > 0].to_markdown())

    report.append("### üì¶ –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏:")
    report.append(df_cleaned.dtypes.to_frame("–¢–∏–ø").to_markdown())

    unique_vals = df_cleaned.nunique()
    report.append("### üß¨ –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —Å—Ç–æ–ª–±—Ü–∞–º:")
    report.append(unique_vals.to_frame("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö").to_markdown())

    return "\n\n".join(report)


def show_data_issues(issues_dict):
    st.subheader("üõ† –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö:")
    for k, v in issues_dict.items():
        st.markdown(f"**{k}:**")
        st.dataframe(v)


# 3. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
def plot_data_visualizations(df, x, y=None, top_n=None, numeric_filters=None, chart_type="–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏"):
    try:
        if y and x == y:
            st.warning("–í—ã –≤—ã–±—Ä–∞–ª–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã –¥–ª—è –æ—Å–µ–π X –∏ Y. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ —Ä–∞–∑–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ.")
            return None

        if numeric_filters:
            for col, (min_val, max_val) in numeric_filters.items():
                df = df[df[col].between(min_val, max_val)]

        if top_n:
            if not pd.api.types.is_numeric_dtype(df[x]):
                top_categories_x = df[x].value_counts().nlargest(top_n).index
                df = df[df[x].isin(top_categories_x)]
            if y and not pd.api.types.is_numeric_dtype(df[y]):
                top_categories_y = df[y].value_counts().nlargest(top_n).index
                df = df[df[y].isin(top_categories_y)]

        x_is_num = pd.api.types.is_numeric_dtype(df[x])
        y_is_num = pd.api.types.is_numeric_dtype(df[y]) if y else False

        # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä—É—á–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ ---
        if chart_type != "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏":
            st.info(f"–í—ã–±—Ä–∞–Ω–Ω—ã–π —Ç–∏–ø –≥—Ä–∞—Ñ–∏–∫–∞: **{chart_type}**")
            if chart_type == "–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞":
                return px.histogram(df, x=x, nbins=30, title=f'Histogram: {x}')
            elif chart_type == "–ö—Ä—É–≥–æ–≤–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞":
                counts = df[x].value_counts()
                if top_n:
                    counts = counts.nlargest(top_n)
                return px.pie(values=counts.values, names=counts.index, title=f'Pie Chart: {x}')
            elif chart_type == "–¢–æ—á–µ—á–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫" and y:
                return px.scatter(df, x=x, y=y, title=f'Scatter: {x} vs {y}')
            elif chart_type == "Boxplot" and y:
                return px.box(df, x=x, y=y, title=f'Boxplot: {x} vs {y}')
            elif chart_type == "Bar-–≥—Ä–∞—Ñ–∏–∫" and y:
                return px.bar(df, x=x, y=y, title=f'Bar: {x} vs {y}')
            else:
                st.warning("–î–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞ –≥—Ä–∞—Ñ–∏–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–∞ –≤—Ç–æ—Ä–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è Y.")
                return None

        # --- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º ---
        if y:
            if x_is_num and y_is_num:
                st.info(f"–ì—Ä–∞—Ñ–∏–∫: Scatter ‚Äî —á–∏—Å–ª–æ–≤–∞—è –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å `{x}` –∏ `{y}`")
                return px.scatter(df, x=x, y=y, title=f'Scatter: {x} vs {y}')

            elif y_is_num and not x_is_num:
                if df[x].nunique() <= 5:
                    st.info(f"–ì—Ä–∞—Ñ–∏–∫: Bar ‚Äî —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ `{y}` –ø–æ `{x}`")
                    agg_df = df.groupby(x)[y].mean().reset_index()
                    return px.bar(agg_df, x=x, y=y, title=f'Bar: {x} vs {y}')
                else:
                    st.info(f"–ì—Ä–∞—Ñ–∏–∫: Boxplot ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ `{y}` –ø–æ `{x}`")
                    return px.box(df, x=x, y=y, title=f'Boxplot: {x} vs {y}')

            elif not x_is_num and not y_is_num:
                st.info(f"–ì—Ä–∞—Ñ–∏–∫: Bar ‚Äî –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ `{x}` –∏ —Ü–≤–µ—Ç –ø–æ `{y}`")
                return px.histogram(df, x=x, color=y, barmode="group", title=f'Bar: {x} by {y}')

            else:
                return px.bar(df, x=x, y=y, title=f'Bar: {x} vs {y}')
        else:
            if x_is_num:
                st.info(f"–ì—Ä–∞—Ñ–∏–∫: Histogram ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π `{x}`")
                return px.histogram(df, x=x, nbins=30, title=f'Histogram: {x}')
            else:
                counts = df[x].value_counts()
                if top_n:
                    counts = counts.nlargest(top_n)
                st.info(f"–ì—Ä–∞—Ñ–∏–∫: Pie ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–æ `{x}`")
                return px.pie(values=counts.values, names=counts.index, title=f'Pie Chart: {x}')

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
        return None
    

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
def train_model(df, target_column, model_type):
    df = df.copy()
    st.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")

    # –†–∞–∑–¥–µ–ª–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    for col in X.select_dtypes(include='object').columns:
        X[col] = LabelEncoder().fit_transform(X[col].astype(str))

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    if X.isnull().any().any() or y.isnull().any():
        st.error(
            "‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∞–Ω–Ω—ã—Ö. "
            "–ú–æ–¥–µ–ª—å –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±—É—á–µ–Ω–∞, –ø–æ–∫–∞ –æ–Ω–∏ –Ω–µ –±—É–¥—É—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã. "
            "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ:\n"
            "- –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Ñ—É–Ω–∫—Ü–∏–µ–π —É–º–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏,\n"
            "- –∏–ª–∏ –≤—ã–±—Ä–∞—Ç—å –¥—Ä—É–≥—É—é —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é,\n"
            "- –∏–ª–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –∑–∞–ø–æ–ª–Ω–∏—Ç—å/—É–¥–∞–ª–∏—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ –≤—Ä—É—á–Ω—É—é."
        )
        return None, None, None, None, None

    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if y.dtype == "object":
        y = LabelEncoder().fit_transform(y.astype(str))

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    if model_type == "Decision Tree":
        model = DecisionTreeClassifier()
    elif model_type == "Logistic Regression":
        model = LogisticRegression()
    elif model_type == "Neural Network":
        model = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42)
    else:
        st.error("‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏.")
        return None, None, None, None, None

    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    metrics = classification_report(y_test, y_pred, output_dict=True)
    st.success("‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")

    return metrics, model, X_test, y_test, y_pred

# 5. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
def plot_predictions(y_test, y_pred):
    st.subheader("üìä –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (Confusion Matrix)")
    cm = confusion_matrix(y_test, y_pred)
    fig, ax = plt.subplots()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax)
    ax.set_xlabel("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ")
    ax.set_ylabel("–ò—Å—Ç–∏–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ")
    st.pyplot(fig)